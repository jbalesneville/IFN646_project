{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd383091-5844-49dd-b6d7-f729e1312a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropBorders( img, l=0.01, r=0.01, u=0.04, d=0.04):\n",
    "\n",
    "    \"\"\"\n",
    "    This function crops a specified percentage of border from\n",
    "    each side of the given image. Default is 1% from the top,\n",
    "    left and right sides and 4% from the bottom side.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : {numpy.ndarray}\n",
    "        The image to crop.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cropped_img: {numpy.ndarray}\n",
    "        The cropped image.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        nrows, ncols = img.shape\n",
    "\n",
    "        # Get the start and end rows and columns\n",
    "        l_crop = int(ncols * l)\n",
    "        r_crop = int(ncols * (1 - r))\n",
    "        u_crop = int(nrows * u)\n",
    "        d_crop = int(nrows * (1 - d))\n",
    "\n",
    "        cropped_img = img[u_crop:d_crop, l_crop:r_crop]\n",
    "\n",
    "    except Exception as e:\n",
    "        # logger.error(f'Unable to cropBorders!\\n{e}')\n",
    "        print((f\"Unable to get cropBorders!\\n{e}\"))\n",
    "\n",
    "    return cropped_img\n",
    "\n",
    "\n",
    "def minMaxNormalise(img):\n",
    "\n",
    "    \"\"\"\n",
    "    This function does min-max normalisation on\n",
    "    the given image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : {numpy.ndarray}\n",
    "        The image to normalise.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    norm_img: {numpy.ndarray}\n",
    "        The min-max normalised image.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        norm_img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "    except Exception as e:\n",
    "        # logger.error(f'Unable to minMaxNormalise!\\n{e}')\n",
    "        print((f\"Unable to get minMaxNormalise!\\n{e}\"))\n",
    "\n",
    "    return norm_img\n",
    "\n",
    "def globalBinarise(img, thresh, maxval):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes in a numpy array image and\n",
    "    returns a corresponding mask that is a global\n",
    "    binarisation on it based on a given threshold\n",
    "    and maxval. Any elements in the array that is\n",
    "    greater than or equals to the given threshold\n",
    "    will be assigned maxval, else zero.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : {numpy.ndarray}\n",
    "        The image to perform binarisation on.\n",
    "    thresh : {int or float}\n",
    "        The global threshold for binarisation.\n",
    "    maxval : {np.uint8}\n",
    "        The value assigned to an element that is greater\n",
    "        than or equals to `thresh`.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    binarised_img : {numpy.ndarray, dtype=np.uint8}\n",
    "        A binarised image of {0, 1}.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        binarised_img = np.zeros(img.shape, np.uint8)\n",
    "        binarised_img[img >= thresh] = maxval\n",
    "\n",
    "    except Exception as e:\n",
    "        # logger.error(f'Unable to globalBinarise!\\n{e}')\n",
    "        print((f\"Unable to globalBinarise!\\n{e}\"))\n",
    "\n",
    "    return binarised_img\n",
    "\n",
    "def editMask(mask, ksize=(23, 23), operation=\"open\"):\n",
    "\n",
    "    \"\"\"\n",
    "    This function edits a given mask (binary image) by performing\n",
    "    closing then opening morphological operations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mask : {numpy.ndarray}\n",
    "        The mask to edit.\n",
    "    ksize : {tuple}\n",
    "        Size of the structuring element.\n",
    "    operation : {str}\n",
    "        Either \"open\" or \"close\", each representing open and close\n",
    "        morphological operations respectively.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    edited_mask : {numpy.ndarray}\n",
    "        The mask after performing close and open morphological\n",
    "        operations.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        kernel = cv2.getStructuringElement(shape=cv2.MORPH_RECT, ksize=ksize)\n",
    "\n",
    "        if operation == \"open\":\n",
    "            edited_mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        elif operation == \"close\":\n",
    "            edited_mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Then dilate\n",
    "        edited_mask = cv2.morphologyEx(edited_mask, cv2.MORPH_DILATE, kernel)\n",
    "\n",
    "    except Exception as e:\n",
    "        # logger.error(f'Unable to editMask!\\n{e}')\n",
    "        print((f\"Unable to get editMask!\\n{e}\"))\n",
    "\n",
    "    return edited_mask\n",
    "\n",
    "def sortContoursByArea(contours, reverse=True):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes in list of contours, sorts them based\n",
    "    on contour area, computes the bounding rectangle for each\n",
    "    contour, and outputs the sorted contours and their\n",
    "    corresponding bounding rectangles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    contours : {list}\n",
    "        The list of contours to sort.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sorted_contours : {list}\n",
    "        The list of contours sorted by contour area in descending\n",
    "        order.\n",
    "    bounding_boxes : {list}\n",
    "        The list of bounding boxes ordered corresponding to the\n",
    "        contours in `sorted_contours`.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Sort contours based on contour area.\n",
    "        sorted_contours = sorted(contours, key=cv2.contourArea, reverse=reverse)\n",
    "\n",
    "        # Construct the list of corresponding bounding boxes.\n",
    "        bounding_boxes = [cv2.boundingRect(c) for c in sorted_contours]\n",
    "\n",
    "    except Exception as e:\n",
    "        # logger.error(f'Unable to sortContourByArea!\\n{e}')\n",
    "        print((f\"Unable to get sortContourByArea!\\n{e}\"))\n",
    "\n",
    "    return sorted_contours, bounding_boxes\n",
    "\n",
    "def xLargestBlobs(mask, top_x=None, reverse=True):\n",
    "\n",
    "    \"\"\"\n",
    "    This function finds contours in the given image and\n",
    "    keeps only the top X largest ones.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mask : {numpy.ndarray, dtype=np.uint8}\n",
    "        The mask to get the top X largest blobs.\n",
    "    top_x : {int}\n",
    "        The top X contours to keep based on contour area\n",
    "        ranked in decesnding order.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    n_contours : {int}\n",
    "        The number of contours found in the given `mask`.\n",
    "    X_largest_blobs : {numpy.ndarray}\n",
    "        The corresponding mask of the image containing only\n",
    "        the top X largest contours in white.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find all contours from binarised image.\n",
    "        # Note: parts of the image that you want to get should be white.\n",
    "        contours, hierarchy = cv2.findContours(\n",
    "            image=mask, mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_NONE\n",
    "        )\n",
    "\n",
    "        n_contours = len(contours)\n",
    "\n",
    "        # Only get largest blob if there is at least 1 contour.\n",
    "        if n_contours > 0:\n",
    "\n",
    "            # Make sure that the number of contours to keep is at most equal\n",
    "            # to the number of contours present in the mask.\n",
    "            if n_contours < top_x or top_x == None:\n",
    "                top_x = n_contours\n",
    "\n",
    "            # Sort contours based on contour area.\n",
    "            sorted_contours, bounding_boxes = sortContoursByArea(\n",
    "                contours=contours, reverse=reverse\n",
    "            )\n",
    "\n",
    "            # Get the top X largest contours.\n",
    "            X_largest_contours = sorted_contours[0:top_x]\n",
    "\n",
    "            # Create black canvas to draw contours on.\n",
    "            to_draw_on = np.zeros(mask.shape, np.uint8)\n",
    "\n",
    "            # Draw contours in X_largest_contours.\n",
    "            X_largest_blobs = cv2.drawContours(\n",
    "                image=to_draw_on,  # Draw the contours on `to_draw_on`.\n",
    "                contours=X_largest_contours,  # List of contours to draw.\n",
    "                contourIdx=-1,  # Draw all contours in `contours`.\n",
    "                color=1,  # Draw the contours in white.\n",
    "                thickness=-1,  # Thickness of the contour lines.\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        # logger.error(f'Unable to xLargestBlobs!\\n{e}')\n",
    "        print((f\"Unable to get xLargestBlobs!\\n{e}\"))\n",
    "\n",
    "    return n_contours, X_largest_blobs\n",
    "\n",
    "def applyMask( img, mask):\n",
    "\n",
    "    \"\"\"\n",
    "    This function applies a mask to a given image. White\n",
    "    areas of the mask are kept, while black areas are\n",
    "    removed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : {numpy.ndarray}\n",
    "        The image to mask.\n",
    "    mask : {numpy.ndarray, dtype=np.uint8}\n",
    "        The mask to apply.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    masked_img: {numpy.ndarray}\n",
    "        The masked image.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        masked_img = img.copy()\n",
    "        masked_img[mask == 0] = 0\n",
    "\n",
    "    except Exception as e:\n",
    "        # logger.error(f'Unable to applyMask!\\n{e}')\n",
    "        print((f\"Unable to get applyMask!\\n{e}\"))\n",
    "\n",
    "    return masked_img\n",
    "\n",
    "def checkLRFlip( mask):\n",
    "\n",
    "    \"\"\"\n",
    "    This function checks whether or not an image needs to be\n",
    "    flipped horizontally (i.e. left-right flip). The correct\n",
    "    orientation is the breast being on the left (i.e. facing\n",
    "    right).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mask : {numpy.ndarray, dtype=np.uint8}\n",
    "        The corresponding mask of the image to flip.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    LR_flip : {boolean}\n",
    "        True means need to flip horizontally,\n",
    "        False means otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Get number of rows and columns in the image.\n",
    "        nrows, ncols = mask.shape\n",
    "        x_center = ncols // 2\n",
    "        y_center = nrows // 2\n",
    "\n",
    "        # Sum down each column.\n",
    "        col_sum = mask.sum(axis=0)\n",
    "        # Sum across each row.\n",
    "        row_sum = mask.sum(axis=1)\n",
    "\n",
    "        left_sum = sum(col_sum[0:x_center])\n",
    "        right_sum = sum(col_sum[x_center:-1])\n",
    "\n",
    "        if left_sum < right_sum:\n",
    "            LR_flip = True\n",
    "        else:\n",
    "            LR_flip = False\n",
    "\n",
    "    except Exception as e:\n",
    "        # logger.error(f'Unable to checkLRFlip!\\n{e}')\n",
    "        print((f\"Unable to get checkLRFlip!\\n{e}\"))\n",
    "\n",
    "    return LR_flip\n",
    "\n",
    "def makeLRFlip( img):\n",
    "\n",
    "    \"\"\"\n",
    "    This function flips a given image horizontally (i.e. left-right).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : {numpy.ndarray}\n",
    "        The image to flip.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    flipped_img : {numpy.ndarray}\n",
    "        The flipped image.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        flipped_img = np.fliplr(img)\n",
    "    except Exception as e:\n",
    "        # logger.error(f'Unable to makeLRFlip!\\n{e}')\n",
    "        print((f\"Unable to get makeLRFlip!\\n{e}\"))\n",
    "\n",
    "    return flipped_img\n",
    "\n",
    "def clahe( img, clip=2.0, tile=(8, 8)):\n",
    "\n",
    "    \"\"\"\n",
    "    This function applies the Contrast-Limited Adaptive\n",
    "    Histogram Equalisation filter to a given image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : {numpy.ndarray}\n",
    "        The image to edit.\n",
    "    clip : {int or floa}\n",
    "        Threshold for contrast limiting.\n",
    "    tile : {tuple (int, int)}\n",
    "        Size of grid for histogram equalization. Input\n",
    "        image will be divided into equally sized\n",
    "        rectangular tiles. `tile` defines the number of\n",
    "        tiles in row and column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clahe_img : {numpy.ndarray, np.uint8}\n",
    "        The CLAHE edited image, with values ranging from [0, 255]\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Convert to uint8.\n",
    "        # img = skimage.img_as_ubyte(img)\n",
    "        img = cv2.normalize(\n",
    "            img,\n",
    "            None,\n",
    "            alpha=0,\n",
    "            beta=255,\n",
    "            norm_type=cv2.NORM_MINMAX,\n",
    "            dtype=cv2.CV_32F,\n",
    "        )\n",
    "        img_uint8 = img.astype(\"uint8\")\n",
    "        #  img = cv2.normalize(\n",
    "        #     img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U\n",
    "        # )\n",
    "\n",
    "        clahe_create = cv2.createCLAHE(clipLimit=clip, tileGridSize=tile)\n",
    "        clahe_img = clahe_create.apply(img_uint8)\n",
    "\n",
    "    except Exception as e:\n",
    "        # logger.error(f'Unable to clahe!\\n{e}')\n",
    "        print((f\"Unable to get clahe!\\n{e}\"))\n",
    "\n",
    "    return clahe_img\n",
    "\n",
    "def pad( img):\n",
    "\n",
    "    \"\"\"\n",
    "    This function pads a given image with black pixels,\n",
    "    along its shorter side, into a square and returns\n",
    "    the square image.\n",
    "\n",
    "    If the image is portrait, black pixels will be\n",
    "    padded on the right to form a square.\n",
    "\n",
    "    If the image is landscape, black pixels will be\n",
    "    padded on the bottom to form a square.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : {numpy.ndarray}\n",
    "        The image to pad.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_img : {numpy.ndarray}\n",
    "        The padded square image, if padding was required\n",
    "        and done.\n",
    "    img : {numpy.ndarray}\n",
    "        The original image, if no padding was required.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        nrows, ncols = img.shape\n",
    "\n",
    "        # If padding is required...\n",
    "        if nrows != ncols:\n",
    "\n",
    "            # Take the longer side as the target shape.\n",
    "            if ncols < nrows:\n",
    "                target_shape = (nrows, nrows)\n",
    "            elif nrows < ncols:\n",
    "                target_shape = (ncols, ncols)\n",
    "\n",
    "            # pad.\n",
    "            padded_img = np.zeros(shape=target_shape)\n",
    "            padded_img[:nrows, :ncols] = img\n",
    "\n",
    "        # If padding is not required...\n",
    "        elif nrows == ncols:\n",
    "\n",
    "            # Return original image.\n",
    "            padded_img = img\n",
    "\n",
    "    except Exception as e:\n",
    "        # logger.error(f'Unable to pad!\\n{e}')\n",
    "        print((f\"Unable to pad!\\n{e}\"))\n",
    "\n",
    "    return padded_img\n",
    "\n",
    "def rescale_image(image, new_size=(112, 112)):\n",
    "    \"\"\"\n",
    "    Rescale an image to the specified size.\n",
    "\n",
    "    Parameters:\n",
    "    - image: Input image of data type CV_32F.\n",
    "    - new_size: Tuple representing the target size (height, width).\n",
    "\n",
    "    Returns:\n",
    "    - Rescaled image.\n",
    "    \"\"\"\n",
    "    # Ensure the image is in the range [0, 1]\n",
    "    image = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "    # Resize the image to the new size\n",
    "    rescaled_image = cv2.resize(image, new_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return rescaled_image\n",
    "\n",
    "import cv2  # Assuming you have cv2 imported\n",
    "\n",
    "def main_preprocess(img):\n",
    "    new_image = cropBorders(img)\n",
    "    norm_image = minMaxNormalise(new_image)\n",
    "    bin_image = globalBinarise(norm_image, 0.1, 1)\n",
    "\n",
    "    # Initialize edited_mask with the output of globalBinarise\n",
    "    edited_mask = editMask(bin_image, ksize=(23, 23), operation=\"open\")\n",
    "\n",
    "    # Process edited_mask further if needed\n",
    "    _, largest_mask = xLargestBlobs(edited_mask, top_x=1, reverse=True)\n",
    "    \n",
    "    masked_img = applyMask(norm_image, largest_mask)\n",
    "    lr_flip = checkLRFlip(mask=largest_mask)\n",
    "\n",
    "    if lr_flip:\n",
    "        flipped_img = makeLRFlip(img=masked_img)\n",
    "    elif not lr_flip:\n",
    "        flipped_img = masked_img\n",
    "\n",
    "    clahe_img = clahe(img=flipped_img, clip=2.0, tile=(8, 8))\n",
    "    padded_img = pad(img=clahe_img)\n",
    "    padded_img = cv2.normalize(\n",
    "        padded_img,\n",
    "        None,\n",
    "        alpha=0,\n",
    "        beta=255,\n",
    "        norm_type=cv2.NORM_MINMAX,\n",
    "        dtype=cv2.CV_32F,\n",
    "    )\n",
    "    rescaled_img = rescale_image(padded_img, new_size=(112, 112 ))\n",
    "    processed_image = cv2.merge((rescaled_img, rescaled_img, rescaled_img))\n",
    "\n",
    "    return processed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569279dd-4a20-4b08-af70-1e2e9c31b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbceda8-593e-4b5c-b311-5f37b6b5acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_image = load_dicom_image(\"C:/Users/jbale/OneDrive - Queensland University of Technology/manifest-1696323463913/CBIS-DDSM/Calc-Test_P_00038_LEFT_CC/08-29-2017-DDSM-NA-96009/1.000000-full mammogram images-63992/P_00038_LEFT_CC_FULL.dcm\")\n",
    "test_img = main_preprocess(dicom_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b295999-f084-46c0-bc3d-42590eaeb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_calc = pd.read_csv(\"C:/Users/jbale/OneDrive - Queensland University of Technology/manifest-1696323463913/calc_case_description_train_set.csv\")\n",
    "test_calc = pd.read_csv(\"C:/Users/jbale/OneDrive - Queensland University of Technology/manifest-1696323463913/calc_case_description_test_set.csv\")\n",
    "\n",
    "train_mass = pd.read_csv(\"C:/Users/jbale/OneDrive - Queensland University of Technology/manifest-1696323463913/mass_case_description_train_set.csv\")\n",
    "test_mass = pd.read_csv(\"C:/Users/jbale/OneDrive - Queensland University of Technology/manifest-1696323463913/mass_case_description_test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09becf0-2b87-4f75-855f-f340877c8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract everything up to the first '/' for file paths\n",
    "train_calc['image file path'] = train_calc['image file path'].str.split('/').str[0]\n",
    "train_calc['cropped image file path'] = train_calc['cropped image file path'].str.split('/').str[0]\n",
    "train_calc['ROI mask file path'] = train_calc['cropped image file path'].str.split('/').str[0]\n",
    "\n",
    "test_calc['image file path'] = test_calc['image file path'].str.split('/').str[0]\n",
    "test_calc['cropped image file path'] = test_calc['cropped image file path'].str.split('/').str[0]\n",
    "test_calc['ROI mask file path'] = test_calc['cropped image file path'].str.split('/').str[0]\n",
    "\n",
    "train_mass['image file path'] = train_mass['image file path'].str.split('/').str[0]\n",
    "train_mass['cropped image file path'] = train_mass['cropped image file path'].str.split('/').str[0]\n",
    "train_mass['ROI mask file path'] = train_mass['cropped image file path'].str.split('/').str[0]                                                                                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44c9c2ef-0f1b-4c9b-9056-96ca10988e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>breast_density</th>\n",
       "      <th>left or right breast</th>\n",
       "      <th>image view</th>\n",
       "      <th>abnormality id</th>\n",
       "      <th>abnormality type</th>\n",
       "      <th>mass shape</th>\n",
       "      <th>mass margins</th>\n",
       "      <th>assessment</th>\n",
       "      <th>pathology</th>\n",
       "      <th>subtlety</th>\n",
       "      <th>image file path</th>\n",
       "      <th>cropped image file path</th>\n",
       "      <th>ROI mask file path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_00001</td>\n",
       "      <td>3</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>mass</td>\n",
       "      <td>IRREGULAR-ARCHITECTURAL_DISTORTION</td>\n",
       "      <td>SPICULATED</td>\n",
       "      <td>4</td>\n",
       "      <td>MALIGNANT</td>\n",
       "      <td>4</td>\n",
       "      <td>Mass-Training_P_00001_LEFT_CC</td>\n",
       "      <td>Mass-Training_P_00001_LEFT_CC_1</td>\n",
       "      <td>Mass-Training_P_00001_LEFT_CC_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P_00001</td>\n",
       "      <td>3</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>1</td>\n",
       "      <td>mass</td>\n",
       "      <td>IRREGULAR-ARCHITECTURAL_DISTORTION</td>\n",
       "      <td>SPICULATED</td>\n",
       "      <td>4</td>\n",
       "      <td>MALIGNANT</td>\n",
       "      <td>4</td>\n",
       "      <td>Mass-Training_P_00001_LEFT_MLO</td>\n",
       "      <td>Mass-Training_P_00001_LEFT_MLO_1</td>\n",
       "      <td>Mass-Training_P_00001_LEFT_MLO_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_00004</td>\n",
       "      <td>3</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>mass</td>\n",
       "      <td>ARCHITECTURAL_DISTORTION</td>\n",
       "      <td>ILL_DEFINED</td>\n",
       "      <td>4</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>3</td>\n",
       "      <td>Mass-Training_P_00004_LEFT_CC</td>\n",
       "      <td>Mass-Training_P_00004_LEFT_CC_1</td>\n",
       "      <td>Mass-Training_P_00004_LEFT_CC_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P_00004</td>\n",
       "      <td>3</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>1</td>\n",
       "      <td>mass</td>\n",
       "      <td>ARCHITECTURAL_DISTORTION</td>\n",
       "      <td>ILL_DEFINED</td>\n",
       "      <td>4</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>3</td>\n",
       "      <td>Mass-Training_P_00004_LEFT_MLO</td>\n",
       "      <td>Mass-Training_P_00004_LEFT_MLO_1</td>\n",
       "      <td>Mass-Training_P_00004_LEFT_MLO_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P_00004</td>\n",
       "      <td>3</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>1</td>\n",
       "      <td>mass</td>\n",
       "      <td>OVAL</td>\n",
       "      <td>CIRCUMSCRIBED</td>\n",
       "      <td>4</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>5</td>\n",
       "      <td>Mass-Training_P_00004_RIGHT_MLO</td>\n",
       "      <td>Mass-Training_P_00004_RIGHT_MLO_1</td>\n",
       "      <td>Mass-Training_P_00004_RIGHT_MLO_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patient_id  breast_density left or right breast image view  abnormality id  \\\n",
       "0    P_00001               3                 LEFT         CC               1   \n",
       "1    P_00001               3                 LEFT        MLO               1   \n",
       "2    P_00004               3                 LEFT         CC               1   \n",
       "3    P_00004               3                 LEFT        MLO               1   \n",
       "4    P_00004               3                RIGHT        MLO               1   \n",
       "\n",
       "  abnormality type                          mass shape   mass margins  \\\n",
       "0             mass  IRREGULAR-ARCHITECTURAL_DISTORTION     SPICULATED   \n",
       "1             mass  IRREGULAR-ARCHITECTURAL_DISTORTION     SPICULATED   \n",
       "2             mass            ARCHITECTURAL_DISTORTION    ILL_DEFINED   \n",
       "3             mass            ARCHITECTURAL_DISTORTION    ILL_DEFINED   \n",
       "4             mass                                OVAL  CIRCUMSCRIBED   \n",
       "\n",
       "   assessment  pathology  subtlety                  image file path  \\\n",
       "0           4  MALIGNANT         4    Mass-Training_P_00001_LEFT_CC   \n",
       "1           4  MALIGNANT         4   Mass-Training_P_00001_LEFT_MLO   \n",
       "2           4     BENIGN         3    Mass-Training_P_00004_LEFT_CC   \n",
       "3           4     BENIGN         3   Mass-Training_P_00004_LEFT_MLO   \n",
       "4           4     BENIGN         5  Mass-Training_P_00004_RIGHT_MLO   \n",
       "\n",
       "             cropped image file path                 ROI mask file path  \n",
       "0    Mass-Training_P_00001_LEFT_CC_1    Mass-Training_P_00001_LEFT_CC_1  \n",
       "1   Mass-Training_P_00001_LEFT_MLO_1   Mass-Training_P_00001_LEFT_MLO_1  \n",
       "2    Mass-Training_P_00004_LEFT_CC_1    Mass-Training_P_00004_LEFT_CC_1  \n",
       "3   Mass-Training_P_00004_LEFT_MLO_1   Mass-Training_P_00004_LEFT_MLO_1  \n",
       "4  Mass-Training_P_00004_RIGHT_MLO_1  Mass-Training_P_00004_RIGHT_MLO_1  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mass.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec80cd43-6b8a-4b90-b5c4-fa8a7da6c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values in the 'pathology' column\n",
    "train_calc['pathology'] = train_calc['pathology'].apply(lambda x: 'BENIGN' if x.startswith('BENIGN') else x)\n",
    "test_calc['pathology'] = test_calc['pathology'].apply(lambda x: 'BENIGN' if x.startswith('BENIGN') else x)\n",
    "\n",
    "train_mass['pathology'] = train_mass['pathology'].apply(lambda x: 'BENIGN' if x.startswith('BENIGN') else x)\n",
    "test_mass['pathology'] = test_mass['pathology'].apply(lambda x: 'BENIGN' if x.startswith('BENIGN') else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0858b12f-e57a-4749-b12e-6ec9fb8b994c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  patient_id  breast_density left or right breast image view  abnormality id  \\\n",
      "0    P_00001               3                 LEFT         CC               1   \n",
      "\n",
      "  abnormality type                          mass shape mass margins  \\\n",
      "0             mass  IRREGULAR-ARCHITECTURAL_DISTORTION   SPICULATED   \n",
      "\n",
      "   assessment  pathology  subtlety                image file path  \\\n",
      "0           4  MALIGNANT         4  Mass-Training_P_00001_LEFT_CC   \n",
      "\n",
      "           cropped image file path               ROI mask file path  \n",
      "0  Mass-Training_P_00001_LEFT_CC_1  Mass-Training_P_00001_LEFT_CC_1  \n"
     ]
    }
   ],
   "source": [
    "print(train_mass.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a25ebd-85c3-4c52-9954-f74bb4982ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fc998aa-4cea-4fc0-80b0-61471fefb506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_preprocess(ROI):\n",
    "    mask = cropBorders(ROI)\n",
    "    mask = minMaxNormalise(mask)\n",
    "    mask = globalBinarise(mask, 0.1, 1)\n",
    "    mask = pad(mask)\n",
    "    mask = rescale_image(mask, new_size=(112, 112))\n",
    "    processed_mask = cv2.merge((mask, mask, mask))\n",
    "\n",
    "    \n",
    "    return processed_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43904e18-6624-4975-a564-286b64dc1ab4",
   "metadata": {},
   "source": [
    "### Trying with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1156be36-b380-4d2d-867e-5fd7a6e1ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "import os\n",
    "dataset_directory = r'C:\\Users\\jbale\\OneDrive - Queensland University of Technology\\manifest-1696323463913\\CBIS-DDSM'\n",
    "batch_size = 16\n",
    "\n",
    "# Get all file paths\n",
    "file_paths = []\n",
    "for root, dirs, files in os.walk(dataset_directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".dcm\"):\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "\n",
    "# Filter file paths to keep only those containing \"FULL\" to filter for mammograms\n",
    "mammogram_paths = [path for path in file_paths if \"FULL\" in os.path.basename(path)]\n",
    "\n",
    "mask_paths = [path for path in file_paths if \"MASK\" in os.path.basename(path)]\n",
    "\n",
    "crop_paths = [path for path in file_paths if \"CROP\" in os.path.basename(path)]\n",
    "\n",
    "\n",
    "# Training loop\n",
    "train_file_paths_mam = [path for path in mammogram_paths if \"Training\" in path]\n",
    "train_file_paths_mask = [path for path in mask_paths if \"Training\" in path]\n",
    "train_file_paths_crop = [path for path in crop_paths if \"Training\" in path]\n",
    "\n",
    "\n",
    "# Testing loop\n",
    "test_file_paths_mam = [path for path in mammogram_paths if \"Test\" in path]\n",
    "test_file_paths_mask = [path for path in mask_paths if \"Test\" in path]\n",
    "test_file_paths_crop = [path for path in crop_paths if \"Test\" in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eabbd64e-7878-48ae-8bcc-45248512f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc= [path for path in train_file_paths_mam if \"Calc\" in path]\n",
    "calc_crop = calc= [path for path in train_file_paths_crop if \"Calc\" in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bca88293-9744-470e-88e7-08ba2e2f66ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass= [path for path in train_file_paths_mam if \"Mass\" in path]\n",
    "mass_crop= [path for path in train_file_paths_crop if \"Mass\" in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d9a194c-77d6-4e73-bbfc-1f4cd2c46a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_test_crop = mass_test= [path for path in test_file_paths_crop if \"Mass\" in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b307474e-e009-4d06-a0f7-d14b4eef5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_test= [path for path in test_file_paths_mam if \"Mass\" in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c23a849-1338-4d36-9a3b-b43666221a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mass_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f3b054f-5b8c-4918-9c37-f1c14c02e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_paths_mass = [os.path.basename(path).split('_FULL')[0] for path in mass]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e18852a-7569-4a25-8170-20a6a1bab78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_paths_mass_test = [os.path.basename(path).split('_FULL')[0] for path in mass_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2c30f62-6a0a-4e07-91f0-ed6c0a27c307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1546"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3596ffd9-7e79-466a-a686-c5433db7bac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1231"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0137576e-0609-46c9-8080-4c67ec92c183",
   "metadata": {},
   "source": [
    "### Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f6d842f-710c-4b60-8194-7ecd18c3d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load DICOM images\n",
    "def load_dicom_image(path):\n",
    "    dcm = pydicom.dcmread(path)\n",
    "    return dcm.pixel_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e7178d2-84d2-499c-a45a-570207d20882",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest',\n",
    "    rescale=1./255  # rescale the image values to be in the range [0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c61cb4-e0b5-4d32-8d4b-a4c5ca21f1c2",
   "metadata": {},
   "source": [
    "## example load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10b2efa3-1d4d-453f-9312-be7d8eb2dee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          File Path  Label\n",
      "0   Mass-Training_P_00475_RIGHT_MLO      0\n",
      "1    Mass-Training_P_00836_LEFT_MLO      1\n",
      "2   Mass-Training_P_00080_RIGHT_MLO      1\n",
      "3    Mass-Training_P_00782_RIGHT_CC      1\n",
      "4   Mass-Training_P_01654_RIGHT_MLO      1\n",
      "5    Mass-Training_P_01250_RIGHT_CC      0\n",
      "6    Mass-Training_P_00430_LEFT_MLO      0\n",
      "7   Mass-Training_P_00313_RIGHT_MLO      1\n",
      "8    Mass-Training_P_01754_RIGHT_CC      1\n",
      "9    Mass-Training_P_01652_RIGHT_CC      1\n",
      "10  Mass-Training_P_00539_RIGHT_MLO      1\n",
      "11   Mass-Training_P_00134_LEFT_MLO      1\n",
      "12   Mass-Training_P_00330_LEFT_MLO      0\n",
      "13  Mass-Training_P_01755_RIGHT_MLO      0\n",
      "14   Mass-Training_P_01641_LEFT_MLO      1\n",
      "15    Mass-Training_P_00298_LEFT_CC      0\n",
      "16  Mass-Training_P_01103_RIGHT_MLO      1\n",
      "17  Mass-Training_P_01280_RIGHT_MLO      0\n",
      "18   Mass-Training_P_00797_RIGHT_CC      0\n",
      "19   Mass-Training_P_00092_LEFT_MLO      1\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_batch_train(mass, labels_df, batch_size=20):\n",
    "    # Create a dictionary to map 'mass' values to 'pathology' labels\n",
    "    path_to_label = dict(zip(labels_df['image file path'], labels_df['pathology']))\n",
    "\n",
    "    data_dict = {\"File Path\": [], \"Label\": []}  # Initialize a dictionary to store data\n",
    "\n",
    "    np.random.shuffle(mass)\n",
    "    batch_paths = mass[:batch_size]  # Take a sample of 25 mass values\n",
    "\n",
    "    for file_path in batch_paths:\n",
    "        # Get the corresponding label based on the matched value from 'mass'\n",
    "        pathology = path_to_label.get(file_path, \"unknown\")\n",
    "\n",
    "        # Map 'BENIGN' to 0 and 'MALIGNANT' to 1\n",
    "        label = 1 if pathology == \"MALIGNANT\" else 0\n",
    "\n",
    "        data_dict[\"File Path\"].append(file_path)\n",
    "        data_dict[\"Label\"].append(label)\n",
    "\n",
    "    return data_dict  # Return the data as a dictionary\n",
    "\n",
    "# Usage example:\n",
    "sample_data = load_and_preprocess_batch_train(transformed_paths_mass,train_mass , batch_size=20)\n",
    "data_df = pd.DataFrame(sample_data)\n",
    "print(data_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf893451-c73d-4ea0-9f0b-ef3a7da2f8ae",
   "metadata": {},
   "source": [
    "## subset (subtlety) load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "280f4a79-e832-4dfa-85ad-ef4f170d0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_batch_train_subtelty(original_mass_train, labels_df, datagen, batch_size=batch_size):\n",
    "    # Create a dictionary to map 'mass' values to 'pathology' labels\n",
    "    path_to_label = dict(zip(labels_df['image file path'], labels_df['pathology']))\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(original_mass_train)\n",
    "        train_file_paths = []  # Initialize the list to store training file paths\n",
    "\n",
    "        for i in range(0, len(original_mass_train), batch_size):\n",
    "            batch_paths = original_mass_train[i:i + batch_size]\n",
    "            image_list = []\n",
    "            labels = []\n",
    "\n",
    "            for file_path in batch_paths:\n",
    "                # Use the original file path here for loading the DICOM image\n",
    "                original_file_path = file_path\n",
    "                \n",
    "                dicom_image = load_dicom_image(original_file_path)\n",
    "\n",
    "                # Apply data augmentation using the provided datagen object\n",
    "                processed_image = main_preprocess(dicom_image)\n",
    "                processed_image = datagen.random_transform(processed_image)\n",
    "\n",
    "                # Transform the file path for label mapping\n",
    "                transformed_path = transform_file_path(original_file_path)\n",
    "\n",
    "                subtlety = labels_df.loc[labels_df['image file path'] == transformed_path]['subtlety'].values\n",
    "                if len(subtlety) > 0:\n",
    "                    subtlety = subtlety[0]\n",
    "                else:\n",
    "                    continue  # Skip this iteration since 'subtelty' is not available for the current file\n",
    "\n",
    "                # Get the 'abnormality id' value\n",
    "                abnormality_id = labels_df.loc[labels_df['image file path'] == transformed_path]['abnormality id'].values\n",
    "                if len(abnormality_id) > 0:\n",
    "                    abnormality_id = abnormality_id[0]\n",
    "                else:\n",
    "                    continue  # Skip this iteration since 'abnormality id' is not available for the current file\n",
    "\n",
    "                pathology = path_to_label.get(transformed_path, \"unknown\")\n",
    "\n",
    "                # Map 'BENIGN' to 0 and 'MALIGNANT' to 1\n",
    "                label = 1 if pathology == \"MALIGNANT\" else 0\n",
    "\n",
    "                # Include only data with 'subtlety' 4 or 5 and 'abnormality id' 1 for training\n",
    "                if subtlety in [4, 5] and abnormality_id == 1:\n",
    "                    image_list.append(processed_image)\n",
    "                    labels.append(label)\n",
    "                    train_file_paths.append(original_file_path)  # Store the file path\n",
    "\n",
    "            if image_list:\n",
    "                yield np.array(image_list), np.array(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ea9d0-c565-4db9-afa2-c4545e4cb611",
   "metadata": {},
   "source": [
    "## subset (subtlety) load function TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b3489-d966-4b23-afc8-e0b937539f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_batch_test_subtelty(original_mass_test, labels_df, batch_size=batch_size):\n",
    "    while True:\n",
    "        # Shuffle the testing data at the beginning of each epoch\n",
    "        np.random.shuffle(original_mass_test)\n",
    "\n",
    "        for i in range(0, len(original_mass_test), batch_size):\n",
    "            batch_paths = original_mass_test[i:i + batch_size]\n",
    "            image_list = []\n",
    "            labels = []\n",
    "\n",
    "            for file_path in batch_paths:\n",
    "                # Use the original file path here for loading the DICOM image\n",
    "                original_file_path = file_path\n",
    "\n",
    "                # Transform the file path for label mapping\n",
    "                transformed_path = transform_file_path(original_file_path)\n",
    "\n",
    "                subtlety = labels_df.loc[labels_df['image file path'] == transformed_path]['subtlety'].values\n",
    "                if len(subtlety) > 0:\n",
    "                    subtlety = subtlety[0]\n",
    "                else:\n",
    "                    continue  # Skip this iteration since 'subtelty' is not available for the current file\n",
    "\n",
    "                # Get the 'abnormality id' value\n",
    "                abnormality_id = labels_df.loc[labels_df['image file path'] == transformed_path]['abnormality id'].values\n",
    "                if len(abnormality_id) > 0:\n",
    "                    abnormality_id = abnormality_id[0]\n",
    "                else:\n",
    "                    continue  # Skip this iteration since 'abnormality id' is not available for the current file\n",
    "\n",
    "                pathology = path_to_label.get(transformed_path, \"unknown\")\n",
    "\n",
    "                # Map 'BENIGN' to 0 and 'MALIGNANT' to 1\n",
    "                label = 1 if pathology == \"MALIGNANT\" else 0\n",
    "\n",
    "                # Include only data with 'subtlety' 4 or 5 and 'abnormality id' 1 for testing\n",
    "                if subtlety in [4, 5] and abnormality_id == 1:\n",
    "                    dicom_image = load_dicom_image(original_file_path)\n",
    "                    processed_image = main_preprocess(dicom_image)\n",
    "                    image_list.append(processed_image)\n",
    "                    labels.append(label)\n",
    "\n",
    "            if image_list:\n",
    "                yield np.array(image_list), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf8215f-1d40-4629-9b69-fe12a64144eb",
   "metadata": {},
   "source": [
    "## generators for subtelty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f43d000-ab93-4e1f-b1be-704120c1584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data generator for training data\n",
    "# split the test set into validation and test sets\n",
    "val_file_paths, test_file_paths = train_test_split(mass_test, test_size=0.5, random_state=42)\n",
    "\n",
    "train_generator_subtelty = load_and_preprocess_batch_train_subtelty(mass, train_mass, datagen, batch_size=batch_size)\n",
    "\n",
    "# Create a data generator for testing data\n",
    "val_generator_subtelty = load_and_preprocess_batch_test_subtelty(val_file_paths, test_mass, batch_size=batch_size)\n",
    "\n",
    "# Create a data generator for testing data\n",
    "test_generator_subtelty = load_and_preprocess_batch_test_subtelty(test_file_paths, test_mass, batch_size=batch_size)\n",
    "\n",
    "# Calculate steps per epoch based on the length of your training and test data and batch size\n",
    "steps_per_epoch_train = len(mass) // batch_size\n",
    "steps_per_epoch_val = len(val_file_paths) // batch_size\n",
    "steps_per_epoch_test = len(test_file_paths) // batch_size\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6010db39-ace5-45c2-bb70-e99fa467fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 4, 4, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d_4   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 23,536,641\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Create ResNet50base model\n",
    "base_model_res = ResNet50(weights='imagenet', include_top=False, input_shape=(112, 112, 3))\n",
    "\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model_res = Sequential([\n",
    "    base_model_res,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification, so the output layer has one neuron with sigmoid activation\n",
    "])\n",
    "\n",
    "\n",
    "custom_learning_rate = 0.001  # adjust this\n",
    "\n",
    "# Create a custom optimizer with the desired learning rate\n",
    "custom_optimizer = Adam(learning_rate=custom_learning_rate)\n",
    "# Compile the model\n",
    "model_res.compile(optimizer=custom_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82346b0a-d604-49a2-aa8d-c72186386fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.9536 - accuracy: 0.4958"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "# Define a callback to display training progress\n",
    "class TrainingProgressCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch + 1}/{self.params['epochs']}, Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}, Val Loss: {logs['val_loss']:.4f}, Val Accuracy: {logs['val_accuracy']:.4f}\")\n",
    "\n",
    "# Create the callback\n",
    "progress_callback = TrainingProgressCallback()\n",
    "\n",
    "# Train the model with the callback\n",
    "history = model_res.fit(\n",
    "    train_generator_subtelty,\n",
    "    steps_per_epoch=steps_per_epoch_train,\n",
    "    validation_data=val_generator_subtelty,\n",
    "    validation_steps=steps_per_epoch_val,\n",
    "    epochs=10,\n",
    "    callbacks=[progress_callback]  # Add the progress callback\n",
    ")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Create empty lists to store true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Loop through the test generator and make predictions\n",
    "for batch in test_generator_subtelty:\n",
    "    images, labels = batch\n",
    "    predictions = model_res.predict(images)\n",
    "    true_labels.extend(labels)\n",
    "    predicted_labels.extend(np.round(predictions).flatten())\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "# Print a classification report\n",
    "report = classification_report(true_labels, predicted_labels, target_names=[\"BENIGN\", \"MALIGNANT\"])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc975c2-d6b1-4ff5-8a37-b24ece36ed0a",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd199d-dfbf-498f-8a29-c62ff2221e7d",
   "metadata": {},
   "source": [
    "### Triplet loss network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a577ffbf-eb44-4442-8c7d-f908036d7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import resnet\n",
    "\n",
    "def generate_triplet_batches(original_mass_train, labels_df, datagen, batch_size=batch_size):\n",
    "    path_to_label = dict(zip(labels_df['image file path'], labels_df['pathology']))\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(original_mass_train)\n",
    "        \n",
    "        for i in range(0, len(original_mass_train), batch_size):\n",
    "            batch_paths = original_mass_train[i:i + batch_size]\n",
    "            anchor_images = []\n",
    "            positive_images = []\n",
    "            negative_images = []\n",
    "            \n",
    "            skip_batch = False  # Flag to skip the current batch\n",
    "            for file_path in batch_paths:\n",
    "                dicom_image = load_dicom_image(file_path)\n",
    "                processed_image = main_preprocess(dicom_image)\n",
    "                processed_image = datagen.random_transform(processed_image)\n",
    "                transformed_path = transform_file_path(file_path)\n",
    "                pathology = path_to_label.get(transformed_path, \"unknown\")\n",
    "                label = 1 if pathology == \"MALIGNANT\" else 0\n",
    "                \n",
    "                if label == 1:  # MALIGNANT\n",
    "                    anchor_images.append(processed_image)\n",
    "                    while True:\n",
    "                        random_index = np.random.randint(len(batch_paths))\n",
    "                        positive_image = load_dicom_image(batch_paths[random_index])\n",
    "                        if random_index != i:\n",
    "                            break\n",
    "                    positive_image = main_preprocess(positive_image)\n",
    "                    positive_image = datagen.random_transform(positive_image)\n",
    "                    positive_images.append(positive_image)\n",
    "                    while True:\n",
    "                        random_index = np.random.randint(len(batch_paths))\n",
    "                        negative_image = load_dicom_image(batch_paths[random_index])\n",
    "                        if random_index != i:\n",
    "                            break\n",
    "                    negative_image = main_preprocess(negative_image)\n",
    "                    negative_image = datagen.random_transform(negative_image)\n",
    "                    negative_images.append(negative_image)\n",
    "                \n",
    "                if not anchor_images or not positive_images or not negative_images:\n",
    "                    # If any of the lists are empty, skip this batch\n",
    "                    skip_batch = True\n",
    "                    break\n",
    "            \n",
    "            if not skip_batch:\n",
    "                yield [np.array(anchor_images), np.array(positive_images), np.array(negative_images)], None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d0f4655-65c9-4f64-bd85-648e2297de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = (112,112)\n",
    "\n",
    "base_cnn = resnet.ResNet50(\n",
    "    weights=\"imagenet\", input_shape=target_shape + (3,), include_top=False\n",
    ")\n",
    "\n",
    "flatten = layers.Flatten()(base_cnn.output)\n",
    "dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
    "dense1 = layers.BatchNormalization()(dense1)\n",
    "dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n",
    "dense2 = layers.BatchNormalization()(dense2)\n",
    "output = layers.Dense(256)(dense2)\n",
    "\n",
    "embedding = Model(base_cnn.input, output, name=\"Embedding\")\n",
    "\n",
    "trainable = False\n",
    "for layer in base_cnn.layers:\n",
    "    if layer.name == \"conv5_block1_out\":\n",
    "        trainable = True\n",
    "    layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef6cf734-0ae2-463e-a693-00f47f2866b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=target_shape + (3,))\n",
    "positive_input = layers.Input(name=\"positive\", shape=target_shape + (3,))\n",
    "negative_input = layers.Input(name=\"negative\", shape=target_shape + (3,))\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(resnet.preprocess_input(anchor_input)),\n",
    "    embedding(resnet.preprocess_input(positive_input)),\n",
    "    embedding(resnet.preprocess_input(negative_input)),\n",
    ")\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57fd6f5e-32be-44ed-aaba-59c14d393f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(f(A) - f(P) - f(A) - f(N) + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8653b4b-509b-4bdc-8ad4-cd673a64dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   6549/Unknown - 68067s 10s/step - loss: 0.5041"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1276/2423820768.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msiamese_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSiameseModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msiamese_network\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msiamese_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msiamese_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerate_triplet_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_and_preprocess_batch_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmass_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_mass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer=optimizers.Adam(0.0001))\n",
    "siamese_model.fit(generate_triplet_batches(mass, train_mass, datagen, batch_size=8), epochs=10, validation_data=load_and_preprocess_batch_test(mass_test, test_mass, batch_size=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f0cd0-5fe1-4672-9080-63c10b14a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Binary Classification Head\n",
    "binary_classification_head = layers.Dense(1, activation=\"sigmoid\")(embedding.output)\n",
    "\n",
    "#Create the Binary Classification Model\n",
    "binary_classification_model = Model(inputs=embedding.input, outputs=binary_classification_head)\n",
    "\n",
    "# Compile the Binary Classification Model\n",
    "binary_classification_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=\"binary_crossentropy\",  # Use binary cross-entropy for binary classification\n",
    "    metrics=[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe644d0-40f6-4201-8bb0-4d50a6389aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_batches(original_mass_train, labels_df, datagen, batch_size=batch_size):\n",
    "    # Create a dictionary to map 'mass' values to 'pathology' labels\n",
    "    path_to_label = dict(zip(labels_df['image file path'], labels_df['pathology']))\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(original_mass_train)\n",
    "        \n",
    "        for i in range(0, len(original_mass_train), batch_size):\n",
    "            batch_paths = original_mass_train[i:i + batch_size]\n",
    "            images = []\n",
    "            labels = []\n",
    "\n",
    "            for file_path in batch_paths:\n",
    "                dicom_image = load_dicom_image(file_path)\n",
    "                processed_image = main_preprocess(dicom_image)\n",
    "                processed_image = datagen.random_transform(processed_image)\n",
    "                transformed_path = transform_file_path(file_path)\n",
    "                pathology = path_to_label.get(transformed_path, \"unknown\")\n",
    "                label = 1 if pathology == \"MALIGNANT\" else 0\n",
    "                \n",
    "                images.append(processed_image)\n",
    "                labels.append(label)\n",
    "\n",
    "            yield np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf916d-549a-4d2e-8a97-a31218294c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c0336-19c4-4450-9dbe-91d83b86031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_test_data, your_test_labels = load_and_preprocess_batch_test(original_mass_test, labels_df, batch_size=batch_size)\n",
    "\n",
    "# Split the test data into validation and test sets\n",
    "validation_data, test_data, validation_labels, test_labels = train_test_split(\n",
    "    your_test_data, your_test_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "binary_classification_model.fit(\n",
    "    x=your_binary_data,  # Input your binary classification data here\n",
    "    y=your_binary_labels,  # Binary labels (0 for benign, 1 for malignant)\n",
    "    epochs=10,  # Adjust as needed\n",
    "    validation_data=(your_validation_data, your_validation_labels),  # Validation data\n",
    ")\n",
    "\n",
    "# Evaluate the model for binary classification\n",
    "binary_classification_model.evaluate(your_test_data, your_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf49e028-a1d6-4b72-b393-18ca82e55cac",
   "metadata": {},
   "source": [
    "## on calcification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8968fb-b0e5-4835-be6f-a139400279e7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
